{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1pXIGDtbyl1reAFwEeOqblaxktqJgqZiW","authorship_tag":"ABX9TyPk0r12ndz2pNc0OE1qeVoe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"504d50ac16934ef08fdbc524438bab3f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c495f5056cf94f938e7a7adfc49b5402","IPY_MODEL_e739b52d78574ad7be555db4c821a64e","IPY_MODEL_34adc26627da44fcbedc74107821f0e1"],"layout":"IPY_MODEL_451f58a71e9545ff83a422a1b018100a"}},"c495f5056cf94f938e7a7adfc49b5402":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_246d9f77a6b74fdeafbd3cbb10626152","placeholder":"​","style":"IPY_MODEL_b223d08de7d14257b1ac79371b83f102","value":"100%"}},"e739b52d78574ad7be555db4c821a64e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa737a32b4443ffb50747f1d5116c0d","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ccecba860e44ebdbe45c4f8d716eec7","value":12}},"34adc26627da44fcbedc74107821f0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19999af8b6ff45f8b6bc4ba7781c554f","placeholder":"​","style":"IPY_MODEL_2a054526674e49bda8990e3df863487e","value":" 12/12 [09:09&lt;00:00, 36.79s/it]"}},"451f58a71e9545ff83a422a1b018100a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246d9f77a6b74fdeafbd3cbb10626152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b223d08de7d14257b1ac79371b83f102":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa737a32b4443ffb50747f1d5116c0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ccecba860e44ebdbe45c4f8d716eec7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19999af8b6ff45f8b6bc4ba7781c554f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a054526674e49bda8990e3df863487e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c3adf20ddf84f1583ebc95a516af50e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8075e43ffaab4758995b42e8f36e331a","IPY_MODEL_cf63346c269d445b82e2672b335005a6","IPY_MODEL_46ec3c7a6a244049a87bbd762a6f6998"],"layout":"IPY_MODEL_17e6e8e4ae43422e9626577d2181e920"}},"8075e43ffaab4758995b42e8f36e331a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab2562d2a7a6496cbda5eb343c84e4dd","placeholder":"​","style":"IPY_MODEL_26a4302858c046cfa47482d94489fbaf","value":"100%"}},"cf63346c269d445b82e2672b335005a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c283b3806f444760b0a012c1a0ffff30","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0684d47c6634de893af51bc078642d3","value":12}},"46ec3c7a6a244049a87bbd762a6f6998":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ab6977a14841d3a3fd5e0f29b613c8","placeholder":"​","style":"IPY_MODEL_e8f29b72daba4475b62c7b325932ae0b","value":" 12/12 [08:55&lt;00:00, 37.05s/it]"}},"17e6e8e4ae43422e9626577d2181e920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2562d2a7a6496cbda5eb343c84e4dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26a4302858c046cfa47482d94489fbaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c283b3806f444760b0a012c1a0ffff30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0684d47c6634de893af51bc078642d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30ab6977a14841d3a3fd5e0f29b613c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8f29b72daba4475b62c7b325932ae0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5379f1fba7b401a93f8650f415af710":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2aae2ca364eb4c88b5537fe1fdf412ed","IPY_MODEL_3b3b133a101d4de1b8c86ad80d827e59","IPY_MODEL_87d5c0e308124f7a90a915e933ef14d4"],"layout":"IPY_MODEL_e3b66815ab9f41cb82f4e78f4a33c19d"}},"2aae2ca364eb4c88b5537fe1fdf412ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23565352967640b19a79e62ab8ce2ec1","placeholder":"​","style":"IPY_MODEL_dbea71efa0424cb5bb8003d1b5cdb457","value":"100%"}},"3b3b133a101d4de1b8c86ad80d827e59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0099cb24f00449faa7c4ac8e0b3ebd79","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9bce52c713c94c739bd8bc720e081190","value":12}},"87d5c0e308124f7a90a915e933ef14d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f78ae98f3842ed9e957376d2ae8986","placeholder":"​","style":"IPY_MODEL_a3ee24dcdf05410bbf5527285e68ddff","value":" 12/12 [08:44&lt;00:00, 36.00s/it]"}},"e3b66815ab9f41cb82f4e78f4a33c19d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23565352967640b19a79e62ab8ce2ec1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbea71efa0424cb5bb8003d1b5cdb457":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0099cb24f00449faa7c4ac8e0b3ebd79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bce52c713c94c739bd8bc720e081190":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4f78ae98f3842ed9e957376d2ae8986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3ee24dcdf05410bbf5527285e68ddff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5199232f55714597b73e986485c5d950":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd74ad7e143d4a1c9ddc1482e709c0c8","IPY_MODEL_da402e77bd6d464dbafe19bde02624fc","IPY_MODEL_39adc34e3f634f12b8e3a7290ab9a431"],"layout":"IPY_MODEL_d538628820be4cd48f98bc1c7a7688f4"}},"dd74ad7e143d4a1c9ddc1482e709c0c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4519a3d650b54b95a366e5eb684679a2","placeholder":"​","style":"IPY_MODEL_d747c3f168c34bafba6caa370feaa3eb","value":"100%"}},"da402e77bd6d464dbafe19bde02624fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2ca5965a947484b9c93a48a797bc78b","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc02b1eef37c45cc9d2ebe28f088fabe","value":12}},"39adc34e3f634f12b8e3a7290ab9a431":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74d5cdbae1f7425b8cd65148402048e6","placeholder":"​","style":"IPY_MODEL_99d4be7654af462c963c56f14c384e33","value":" 12/12 [08:41&lt;00:00, 36.34s/it]"}},"d538628820be4cd48f98bc1c7a7688f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4519a3d650b54b95a366e5eb684679a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d747c3f168c34bafba6caa370feaa3eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2ca5965a947484b9c93a48a797bc78b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc02b1eef37c45cc9d2ebe28f088fabe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74d5cdbae1f7425b8cd65148402048e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99d4be7654af462c963c56f14c384e33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"be338634"},"source":["# Task\n","Summarize the performance of the BERT-based depression detection system using the calculated accuracy, precision, recall, and F1-score. Discuss potential improvements and next steps for deployment based on these results and the confusion matrix."]},{"cell_type":"markdown","metadata":{"id":"70ede164"},"source":["## Load Dataset\n","\n","### Subtask:\n","Load the 'indian_depression_dataset_2025.csv' dataset into a Pandas DataFrame and inspect its structure, including columns and initial rows. Identify the text and label columns.\n"]},{"cell_type":"markdown","metadata":{"id":"a4e0abc1"},"source":["**Reasoning**:\n","First, I'll import the pandas library, then load the CSV file 'indian_depression_dataset_2025.csv' into a DataFrame named `df`, and finally display the first 5 rows to inspect its structure and identify the relevant columns.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"824114ff","executionInfo":{"status":"error","timestamp":1765177655706,"user_tz":-330,"elapsed":15,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"ca572eba-9e0f-4900-ed04-1597989f0c69"},"source":["import pandas as pd\n","\n","df = pd.read_csv('indian_depression_dataset_2025.csv')\n","print(df.head())"],"execution_count":19,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'indian_depression_dataset_2025.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1857644309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'indian_depression_dataset_2025.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'indian_depression_dataset_2025.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"1f4d68fe"},"source":["**Reasoning**:\n","The previous code failed because the specified file 'indian_depression_dataset_2025.csv' was not found. I will correct the filename to the available file '/content/raw_depression_dataset_india.csv' to successfully load the dataset.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2d0bfa1","executionInfo":{"status":"ok","timestamp":1765177663284,"user_tz":-330,"elapsed":11,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"b2fa8a0d-371b-4bb6-913c-3ba947f025cb"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/raw_depression_dataset_india.csv')\n","print(df.head())"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["                                          tweet_text  label        date\n","0  my anxiety is at an all time high. i feel like...      1  2025-12-11\n","1  nothing brings me joy anymore. not even the th...      1  2025-12-11\n","2  honestly why am i even trying anymore. nothing...      1  2025-12-02\n","3  The pollution in Delhi is so bad i can barely ...      0  2025-12-05\n","4  my anxiety is at an all time high. i feel like...      1  2025-12-02\n"]}]},{"cell_type":"markdown","metadata":{"id":"b45e02fa"},"source":["## Prepare Data for BERT\n","\n","### Subtask:\n","Preprocess the text data for BERT by tokenizing it, converting tokens to input IDs, creating attention masks, and segment IDs. Split the dataset into training and validation sets to prepare for model training.\n"]},{"cell_type":"markdown","metadata":{"id":"5cd1de9b"},"source":["**Reasoning**:\n","To prepare the text data for BERT, I need to tokenize the 'tweet_text' column, convert the tokens and labels into PyTorch tensors, create a TensorDataset, split it into training and validation sets, and finally create DataLoaders for efficient batch processing during model training.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6c4d88fe","executionInfo":{"status":"ok","timestamp":1765177705300,"user_tz":-330,"elapsed":764,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"5a339d66-3222-44c0-8cc3-b637395e43b2"},"source":["import torch\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","\n","# 1. Import and instantiate BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","# 2. Define MAX_LEN\n","MAX_LEN = 128\n","\n","# 3. Tokenize the 'tweet_text' column\n","input_ids = []\n","attention_masks = []\n","\n","for text in df.tweet_text.values:\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = MAX_LEN,      # Pad & truncate all sentences.\n","                        padding = 'max_length',\n","                        return_attention_mask = True,   # Construct attention masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                        truncation=True\n","                   )\n","\n","    # Add the encoded sentence to the list\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # Add the attention mask to the list\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","# 5. Convert the 'label' column to a PyTorch tensor\n","labels = torch.tensor(df.label.values)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', df.tweet_text.values[0])\n","print('Token IDs:', input_ids[0])\n","\n","# 6. Create a TensorDataset\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# 7. Calculate train and validation set sizes\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","print(f\"Dataset size: {len(dataset)}\")\n","print(f\"Training size: {train_size}\")\n","print(f\"Validation size: {val_size}\")\n","\n","# 8. Use random_split to divide the dataset\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# 10. Define batch_size\n","batch_size = 32 # A common BERT fine-tuning batch size\n","\n","# 11. Create DataLoader instances\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset,    # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pulls batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","print(\"Data preparation complete. Dataloaders for training and validation are created.\")"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  my anxiety is at an all time high. i feel like i can't breathe right now.\n","Token IDs: tensor([  101,  2026, 10089,  2003,  2012,  2019,  2035,  2051,  2152,  1012,\n","         1045,  2514,  2066,  1045,  2064,  1005,  1056,  7200,  2157,  2085,\n","         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","Dataset size: 400\n","Training size: 360\n","Validation size: 40\n","Data preparation complete. Dataloaders for training and validation are created.\n"]}]},{"cell_type":"markdown","metadata":{"id":"fe20c4ba"},"source":["## Load Pre-trained BERT Model\n","\n","### Subtask:\n","Load a pre-trained BERT model suitable for sequence classification from a library like Hugging Face Transformers. Configure the model for a binary classification task (labels 0 and 1).\n"]},{"cell_type":"markdown","metadata":{"id":"837cc942"},"source":["**Reasoning**:\n","To load a pre-trained BERT model, I will import `BertForSequenceClassification` from the `transformers` library, instantiate it for binary classification (`num_labels=2`), and then move the model to the available GPU if one exists, otherwise to the CPU.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ed31bc4","executionInfo":{"status":"ok","timestamp":1765177721008,"user_tz":-330,"elapsed":270,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"2072dafb-16a5-4501-91b8-e7ea240e019c"},"source":["from transformers import BertForSequenceClassification\n","\n","# Check for GPU and set device\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","# Instantiate the BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.\n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU or CPU\n","model.to(device)\n","\n","print(\"BERT model loaded and configured for binary classification.\")"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU available, using the CPU instead.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["BERT model loaded and configured for binary classification.\n"]}]},{"cell_type":"markdown","metadata":{"id":"8ee53365"},"source":["**Reasoning**:\n","The previous code successfully loaded the BERT model, and now the next logical step is to set up the optimizer and learning rate scheduler required for fine-tuning the model. This will involve defining the AdamW optimizer and a linear scheduler with a warmup period.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"6e1912b6","executionInfo":{"status":"error","timestamp":1765177729935,"user_tz":-330,"elapsed":15,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"959463b4-a851-4566-9238-56a9586b3329"},"source":["## Define Optimizer and Learning Rate Scheduler\n","\n","### Subtask:\n","Define the optimizer and a learning rate scheduler for fine-tuning the BERT model. This includes specifying the optimizer's parameters and setting up a scheduler for adjusting the learning rate over time.\n","\n","#### Instructions:\n","1. Import the `AdamW` optimizer from the `transformers` library.\n","2. Import `get_linear_schedule_with_warmup` from `transformers.optimization`.\n","3. Instantiate the `AdamW` optimizer, specifying the model's parameters and a learning rate (e.g., 2e-5) and weight decay.\n","4. Calculate the total number of training steps, typically `(number of epochs * number of training batches)`.\n","5. Create the learning rate scheduler using `get_linear_schedule_with_warmup`, passing the optimizer, number of warmup steps, and total training steps."],"execution_count":23,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unterminated string literal (detected at line 4) (ipython-input-1036446461.py, line 4)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1036446461.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Define the optimizer and a learning rate scheduler for fine-tuning the BERT model. This includes specifying the optimizer's parameters and setting up a scheduler for adjusting the learning rate over time.\u001b[0m\n\u001b[0m                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"00969a60"},"source":["**Reasoning**:\n","The previous code block caused a `SyntaxError` because it contained markdown text instead of executable Python code. I need to replace it with the actual Python implementation for defining the optimizer and learning rate scheduler, following the instructions in the preceding text cells.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":378},"id":"8522acc3","executionInfo":{"status":"error","timestamp":1765177748216,"user_tz":-330,"elapsed":23,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"963e82f0-03b8-4a3c-c47d-9c6fa3ae5184"},"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","# Instantiate the AdamW optimizer with model parameters and learning rate\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,  # Default learning rate\n","                  eps = 1e-8  # Default epsilon\n","                 )\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4 # This value is already present in the kernel state.\n","\n","# Total number of training steps is [number of batches] x [number of epochs].\n","# Calculate total steps based on the train_dataloader size and epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","print(\"Optimizer and learning rate scheduler defined.\")"],"execution_count":24,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3247513807.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Instantiate the AdamW optimizer with model parameters and learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer = AdamW(model.parameters(),\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"232efba3"},"source":["**Reasoning**:\n","The previous error indicated that `AdamW` could not be imported directly from `transformers`. This is because in more recent versions, `AdamW` is typically found in `torch.optim`. I will update the import statement to reflect this change, ensuring that `AdamW` is imported from `torch.optim` while `get_linear_schedule_with_warmup` is still imported from `transformers`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6ca9e40","executionInfo":{"status":"ok","timestamp":1765177759794,"user_tz":-330,"elapsed":14,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"08ee94ae-c388-42de-e293-e362ede53e69"},"source":["from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import torch\n","\n","# Instantiate the AdamW optimizer with model parameters and learning rate\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,  # Default learning rate\n","                  eps = 1e-8  # Default epsilon\n","                 )\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4 # This value is already present in the kernel state.\n","\n","# Total number of training steps is [number of batches] x [number of epochs].\n","# Calculate total steps based on the train_dataloader size and epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","print(\"Optimizer and learning rate scheduler defined.\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimizer and learning rate scheduler defined.\n"]}]},{"cell_type":"markdown","metadata":{"id":"cae7a8db"},"source":["## Fine-tune BERT Model\n","\n","### Subtask:\n","Set up a training loop to fine-tune the loaded BERT model on your prepared training dataset. Define an optimizer, a loss function, and train the model for a specified number of epochs, monitoring validation performance.\n"]},{"cell_type":"markdown","metadata":{"id":"daa06217"},"source":["**Reasoning**:\n","I need to set up the training loop for the BERT model, which involves importing `tqdm`, defining an accuracy calculation function, setting random seeds for reproducibility, and then implementing the per-epoch training and validation steps as outlined in the instructions.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":791,"referenced_widgets":["504d50ac16934ef08fdbc524438bab3f","c495f5056cf94f938e7a7adfc49b5402","e739b52d78574ad7be555db4c821a64e","34adc26627da44fcbedc74107821f0e1","451f58a71e9545ff83a422a1b018100a","246d9f77a6b74fdeafbd3cbb10626152","b223d08de7d14257b1ac79371b83f102","aaa737a32b4443ffb50747f1d5116c0d","8ccecba860e44ebdbe45c4f8d716eec7","19999af8b6ff45f8b6bc4ba7781c554f","2a054526674e49bda8990e3df863487e","7c3adf20ddf84f1583ebc95a516af50e","8075e43ffaab4758995b42e8f36e331a","cf63346c269d445b82e2672b335005a6","46ec3c7a6a244049a87bbd762a6f6998","17e6e8e4ae43422e9626577d2181e920","ab2562d2a7a6496cbda5eb343c84e4dd","26a4302858c046cfa47482d94489fbaf","c283b3806f444760b0a012c1a0ffff30","c0684d47c6634de893af51bc078642d3","30ab6977a14841d3a3fd5e0f29b613c8","e8f29b72daba4475b62c7b325932ae0b","a5379f1fba7b401a93f8650f415af710","2aae2ca364eb4c88b5537fe1fdf412ed","3b3b133a101d4de1b8c86ad80d827e59","87d5c0e308124f7a90a915e933ef14d4","e3b66815ab9f41cb82f4e78f4a33c19d","23565352967640b19a79e62ab8ce2ec1","dbea71efa0424cb5bb8003d1b5cdb457","0099cb24f00449faa7c4ac8e0b3ebd79","9bce52c713c94c739bd8bc720e081190","e4f78ae98f3842ed9e957376d2ae8986","a3ee24dcdf05410bbf5527285e68ddff","5199232f55714597b73e986485c5d950","dd74ad7e143d4a1c9ddc1482e709c0c8","da402e77bd6d464dbafe19bde02624fc","39adc34e3f634f12b8e3a7290ab9a431","d538628820be4cd48f98bc1c7a7688f4","4519a3d650b54b95a366e5eb684679a2","d747c3f168c34bafba6caa370feaa3eb","d2ca5965a947484b9c93a48a797bc78b","fc02b1eef37c45cc9d2ebe28f088fabe","74d5cdbae1f7425b8cd65148402048e6","99d4be7654af462c963c56f14c384e33"]},"id":"e0cff4cc","executionInfo":{"status":"ok","timestamp":1765179989144,"user_tz":-330,"elapsed":2201540,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"38d2c78d-64f7-4c81-dfc3-324ee5f58b36"},"source":["import numpy as np\n","import time\n","import datetime\n","from tqdm.notebook import tqdm\n","\n","# Function to calculate accuracy for a batch\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","# Set the random seed for reproducibility\n","seed_val = 42\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","\n","    print(f'======== Epoch {epoch_i + 1} / {epochs} ========\\nTraining...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode.\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = datetime.timedelta(seconds=int(round(time.time() - t0)))\n","            # Report progress.\n","            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n","\n","        # Unpack this training batch from our dataloader.\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a backward pass.\n","        model.zero_grad()\n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here:\n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different things depending on the content of its arguments.\n","        # In this case, we just need the `loss` (first element) and `logits` (second element).\n","        outputs = model(b_input_ids,\n","                        token_type_ids=None,\n","                        attention_mask=b_input_mask,\n","                        labels=b_labels)\n","\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a `torch.Tensor` containing a single value;\n","        # the `.item()` function just returns the Python value from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the learning rate and how to apply the gradients.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","\n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(f'  Average training loss: {avg_train_loss:.2f}')\n","    print(f'  Training epoch took: {datetime.timedelta(seconds=int(round(time.time() - t0)))}')\n","\n","    print('\\nValidation...')\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n","    model.eval()\n","\n","    # Tracking variables\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have not provided labels.\n","            outputs = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels=b_labels)\n","\n","        # Get the loss and logits from the output\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        # Accumulate the validation loss.\n","        eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and add to the total.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(f'  Validation Loss: {(eval_loss/nb_eval_steps):.2f}')\n","    print(f'  Accuracy: {(eval_accuracy/nb_eval_steps):.2f}')\n","    print(f'  Validation took: {datetime.timedelta(seconds=int(round(time.time() - t0)))}')\n","\n","print('\\nTraining complete!')"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["======== Epoch 1 / 4 ========\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504d50ac16934ef08fdbc524438bab3f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Average training loss: 0.48\n","  Training epoch took: 0:09:09\n","\n","Validation...\n","  Validation Loss: 0.31\n","  Accuracy: 1.00\n","  Validation took: 0:00:19\n","======== Epoch 2 / 4 ========\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3adf20ddf84f1583ebc95a516af50e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Average training loss: 0.24\n","  Training epoch took: 0:08:55\n","\n","Validation...\n","  Validation Loss: 0.15\n","  Accuracy: 1.00\n","  Validation took: 0:00:17\n","======== Epoch 3 / 4 ========\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5379f1fba7b401a93f8650f415af710"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Average training loss: 0.13\n","  Training epoch took: 0:08:45\n","\n","Validation...\n","  Validation Loss: 0.08\n","  Accuracy: 1.00\n","  Validation took: 0:00:17\n","======== Epoch 4 / 4 ========\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5199232f55714597b73e986485c5d950"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Average training loss: 0.08\n","  Training epoch took: 0:08:42\n","\n","Validation...\n","  Validation Loss: 0.06\n","  Accuracy: 1.00\n","  Validation took: 0:00:18\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","metadata":{"id":"11b0d4f9"},"source":["## Evaluate Model Performance\n","\n","### Subtask:\n","Evaluate the fine-tuned BERT model on the validation set. Calculate and display key classification metrics such as accuracy, precision, recall, and F1-score to assess the model's performance. Also, generate a confusion matrix to visualize the true positives, true negatives, false positives, and false negatives.\n"]},{"cell_type":"markdown","metadata":{"id":"cdf77b90"},"source":["**Reasoning**:\n","The previous steps completed the training of the BERT model. Now, to evaluate its performance as per the subtask, I need to collect predictions and true labels from the validation set and then calculate various classification metrics and generate a confusion matrix.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43ecd756","executionInfo":{"status":"ok","timestamp":1765180032745,"user_tz":-330,"elapsed":17936,"user":{"displayName":"Debanjan Halder","userId":"06920941981545673671"}},"outputId":"132add92-5e2b-4840-a2b5-9f26c53bab35"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","# Tracking variables for predictions and true labels\n","val_preds_all = []\n","val_labels_all = []\n","\n","# Predict\n","for batch in validation_dataloader:\n","    # Add batch to device\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions.\n","        outputs = model(b_input_ids,\n","                        token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","\n","    logits = outputs.logits\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    val_preds_all.extend(np.argmax(logits, axis=1).flatten())\n","    val_labels_all.extend(label_ids.flatten())\n","\n","# Convert to numpy arrays\n","val_preds_all = np.array(val_preds_all)\n","val_labels_all = np.array(val_labels_all)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(val_labels_all, val_preds_all)\n","precision = precision_score(val_labels_all, val_preds_all, average='binary', zero_division=0)\n","recall = recall_score(val_labels_all, val_preds_all, average='binary', zero_division=0)\n","f1 = f1_score(val_labels_all, val_preds_all, average='binary', zero_division=0)\n","cm = confusion_matrix(val_labels_all, val_preds_all)\n","\n","print(f\"\\n----- Model Performance on Validation Set -----\")\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"\\nConfusion Matrix:\\n{cm}\")\n","\n","# Interpreting the confusion matrix (assuming label 1 is 'depressed' and 0 is 'not depressed')\n","# cm[0,0]: True Negatives (Correctly predicted not depressed)\n","# cm[0,1]: False Positives (Incorrectly predicted depressed - Type I error)\n","# cm[1,0]: False Negatives (Incorrectly predicted not depressed - Type II error)\n","# cm[1,1]: True Positives (Correctly predicted depressed)\n","\n","print(\"\\nInterpretation of Confusion Matrix:\")\n","print(f\"True Negatives (TN): {cm[0,0]} (Correctly predicted not depressed)\")\n","print(f\"False Positives (FP): {cm[0,1]} (Incorrectly predicted depressed)\")\n","print(f\"False Negatives (FN): {cm[1,0]} (Incorrectly predicted not depressed)\")\n","print(f\"True Positives (TP): {cm[1,1]} (Correctly predicted depressed)\")"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","----- Model Performance on Validation Set -----\n","Accuracy: 1.0000\n","Precision: 1.0000\n","Recall: 1.0000\n","F1 Score: 1.0000\n","\n","Confusion Matrix:\n","[[19  0]\n"," [ 0 21]]\n","\n","Interpretation of Confusion Matrix:\n","True Negatives (TN): 19 (Correctly predicted not depressed)\n","False Positives (FP): 0 (Incorrectly predicted depressed)\n","False Negatives (FN): 0 (Incorrectly predicted not depressed)\n","True Positives (TP): 21 (Correctly predicted depressed)\n"]}]},{"cell_type":"markdown","metadata":{"id":"c2228d8a"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the performance of the depression detection system, including the key evaluation metrics and discuss potential next steps for improvement or deployment.\n"]},{"cell_type":"markdown","metadata":{"id":"739e3798"},"source":["## Summary:\n","\n","### Q&A\n","The BERT-based depression detection system achieved perfect performance on the validation set, with an accuracy of 1.0000, precision of 1.0000, recall of 1.0000, and an F1-score of 1.0000. The confusion matrix showed 19 True Negatives, 0 False Positives, 0 False Negatives, and 21 True Positives, indicating that all samples in the validation set were classified correctly.\n","\n","Based on these results, potential improvements and next steps for deployment should focus on rigorous testing and validation with larger, more diverse, and unseen datasets to confirm the model's generalizability and robustness, as perfect scores on a small validation set may indicate issues like data leakage or an overly simplistic evaluation.\n","\n","### Data Analysis Key Findings\n","*   The dataset `raw_depression_dataset_india.csv` was loaded, containing `tweet_text` for analysis and a `label` column for binary classification.\n","*   The text data was preprocessed for BERT, resulting in a dataset of 400 samples, split into a training set of 360 samples and a validation set of 40 samples.\n","*   A `BertForSequenceClassification` model (`bert-base-uncased`) was loaded and configured for binary classification, fine-tuned using the AdamW optimizer with a learning rate of 2e-5 for 4 epochs on the CPU.\n","*   During fine-tuning, the average training loss decreased from 0.48 in the first epoch to 0.08 in the fourth epoch.\n","*   The validation loss consistently decreased from 0.31 to 0.06 over the four epochs, with validation accuracy consistently reported as 1.00.\n","*   The final evaluation on the validation set yielded:\n","    *   **Accuracy:** 1.0000\n","    *   **Precision:** 1.0000\n","    *   **Recall:** 1.0000\n","    *   **F1 Score:** 1.0000\n","*   The confusion matrix indicated 19 True Negatives, 0 False Positives, 0 False Negatives, and 21 True Positives, meaning the model made no errors on the validation set.\n","\n","### Insights or Next Steps\n","*   The observed perfect performance across all metrics (accuracy, precision, recall, F1-score of 1.0000) on the validation set, especially given its small size of 40 samples, is highly unusual. This suggests a strong possibility of data leakage, an overly simple validation set, or that the dataset is not representative of real-world complexity.\n","*   For deployment and to ensure reliability, it is crucial to re-evaluate the model on a larger, entirely independent, and more diverse test set. Additionally, consider cross-validation techniques or K-fold splitting to better assess the model's generalization capabilities and mitigate risks associated with an extremely high validation score.\n"]}]}